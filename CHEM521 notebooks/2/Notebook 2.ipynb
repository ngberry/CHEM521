{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Workshop 2 - Developing and Assessing Quantitative Structure Activity Relationship Machine Learnt Models\n",
    "\n",
    "<strong>Author(s):</strong> Neil Berry, based on work from Jessica A. Nash, The Molecular Sciences Software Institute (https://github.com/MolSSI-Education/) and Dr Sam Chong (https://github.com/drsamchong)\n",
    "\n",
    "## Learning Objectives\n",
    "- Learn how molecules can be represented in a machine readable format\n",
    "- Generate a cheminformatics data set using the RDKit library starting from a list of SMILES codes\n",
    "- Perform Exploratory Data Analysis\n",
    "- Prepare the data\n",
    "- Create and train a linear regression model\n",
    "- Create and train a random forest regression model\n",
    "- Compare the performance between the models\n",
    "- Tune a random forest model for optimal performance \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning (ML)\n",
    "\n",
    "ML can be applied for (text adapted from [scikit-learn page](http://scikit-learn.org/stable/)):\n",
    "\n",
    "* **Regression**: Prediction of a continuous-values attribute associated with an object - see below\n",
    "* Classification (supervised): Identify which category an object belongs to - see Workshop 3\n",
    "* Clustering (unsupervised): Automated grouping of similar objects into sets - not covered in this course"
   ]
  },
  {
   "attachments": {
    "ML_overview.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "To successfully apply ML, we need a large data set of molecules, a molecular encoding, a label per molecule in the data set, and a ML algorithm to train a model. Then, we can make predictions for new molecules.\n",
    "\n",
    "![ML_overview.png](attachment:ML_overview.png)\n",
    "\n",
    "_Figure 1_: Machine learning overview: Molecular encoding, label, ML algorithm, prediction. Figure by Andrea Volkamer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†If you do not have scikit-learn installed, uncomment the following line\n",
    "# !conda install -y -c conda-forge scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:51:46.314577Z",
     "start_time": "2024-06-26T16:51:46.301095Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd                 # for data manipulation\n",
    "import seaborn as sns               # for data visualization\n",
    "import matplotlib.pyplot as plt     # for data visualization\n",
    "import numpy as np                  # for numerical operations\n",
    "\n",
    "from rdkit import Chem              # for calculating cheminformatics properties of molecules\n",
    "from rdkit.Chem import Descriptors  # for determining chemical descriptors\n",
    "from rdkit.Chem import Crippen      # for calculating logP (cLogP)\n",
    "from rdkit.Chem import PandasTools  # for displaying molecules\n",
    "PandasTools.RenderImagesInAllDataFrames(images=True) # Ensures molecules are rendered in the notebook\n",
    "PandasTools.RenderImagesInAllDataFrames()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler            # for scaling the data\n",
    "from sklearn.model_selection import train_test_split        # for splitting the data into training and testing sets\n",
    "from sklearn.dummy import DummyRegressor                    # for creating a dummy regression\n",
    "from sklearn.linear_model import LinearRegression           # for creating a linear regression model\n",
    "from sklearn.ensemble import RandomForestRegressor          # for creating a random forest regression model\n",
    "from sklearn.metrics import mean_squared_error, r2_score    # for evaluating the model\n",
    "from sklearn.pipeline import make_pipeline                  # for building operational pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note</b>\n",
    "    We have added comments to clarify the purpose of each imported library - please take note. You will be using many of these again.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be **creating a cheminformatics data set** from a machine-readable list of molecules. The goal is to use the provided molecules to calculate various chemical properties of each molecule and then predict the solubility of a molecule base on its chemical structure using machine learning models. We will create a version of the [Delaney's solubility dataset](https://doi.org/10.1021/ci034243x).  We will then use this data set for **building both Linear and Random Forest regression models** and compare their  performance. \n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. Load the data\n",
    "2. Calculate descriptors\n",
    "3. Perform some EDA to gain initial understanding of the distribution of features and relationships between features, and with the target.\n",
    "\n",
    "For each model (may require additional stages depending on the model)\n",
    "\n",
    "4. Prepare the data \n",
    "5. Train the model\n",
    "6. Make predictions\n",
    "7. Evaluate performance\n",
    "8. Analyse the performance of the models. Draw conclusions about the chemical problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDKit \n",
    "\n",
    "Reminder:\n",
    "\n",
    "There are Python libraries that are made for working just with chemical data. One commonly used library in Python for cheminformatics is called [RDKit](https://en.wikipedia.org/wiki/RDKit). RDKit is an open-source cheminformatics library, primarily developed in C++ and has been under development since the year 2000. We will be using the Python interface to RDKit, though there are interfaces in other languages. We used RDKit in the first workshop.\n",
    "\n",
    "RDKit provides a molecule object that allows you to manipulate chemical structures. It has capabilities for reading and writing molecular file formats, calculating molecular properties, and performing substructure searches. In addition, it offers a wide range of cheminformatics algorithms such as molecular fingerprint generation, similarity metrics calculation, and molecular descriptor computation. This notebook introduces RDKit basics.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "<strong>Python Skills: Python Objects</strong>\n",
    "\n",
    "Most of this functionality is achieved through the RDKit `mol` object. In Python, we use the word \"object\" to refer to a variable type with associated data and methods. \n",
    "One example of an object we have seen in notebooks is a list - we could also call it a \"list object\". An object has `attributes` (data) and `methods`. \n",
    "You access information about objects with the syntax\n",
    "```python\n",
    "object.data\n",
    "```\n",
    "where data is the attribute name.\n",
    "\n",
    "You access object methods with the syntax\n",
    "```python\n",
    "object.method(arguments)\n",
    "```\n",
    "</div>    \n",
    "\n",
    "We will create and manipulate RDKit `mol` objects. RDKit `mol` objects represent molecules and have attributes (data) and methods (actions) associated with molecules.\n",
    "\n",
    "We are going to use a part of RDKit called `Chem`. To use `Chem` we have to import it, which we did above in the importing libraries section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning (ML)\n",
    "\n",
    "ML can be applied for (text adapted from [scikit-learn page](http://scikit-learn.org/stable/)):\n",
    "\n",
    "* **Regression**: Prediction of a continuous-values attribute associated with an object - see below\n",
    "* Classification (supervised): Identify which category an object belongs to - see notebook 3\n",
    "* Clustering (unsupervised): Automated grouping of similar objects into sets - not covered in this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The original data set\n",
    "\n",
    "Let's load the list of molecules using the ``pandas`` library \n",
    "and take a look at a few samples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:49:13.095479Z",
     "start_time": "2024-06-26T16:49:13.088117Z"
    }
   },
   "outputs": [],
   "source": [
    "# TO DO Change the data_path variable to the location of solubility-molecule-list.csv \n",
    "# Path to the list of molecules data file\n",
    "data_path = \"\"\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# TO DO Type 10 in the brackets in the command below to see the top 10 lines in the file\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following columns:\n",
    "- **Compound ID**: compound name in a range of formats\n",
    "- **smiles**: SMILES string representation of each molecule\n",
    "- **logS**: the solubility of the molecule in mol/L measured at 25 $\\degree$ ùê∂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding molecule structures to the data set using the SMILES strings\n",
    "\n",
    "Above we werer reminded about molecular representations using SMILES strings. Now we will use SMILES strings to create molecule objects in RDKit. \n",
    "\n",
    "We can create a representation of methane using RDKit by using the `MolFromSmiles` function in `rdkit.Chem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing methane as an example\n",
    "methane = Chem.MolFromSmiles(\"C\")\n",
    "methane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO Type CCC in between the \"\" in the command below\n",
    "# visualizing propane as another example\n",
    "propane = Chem.MolFromSmiles(\"\")\n",
    "propane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The `.apply` function\n",
    "\n",
    "In the examples above, we made a single molecule object from a single SMILES string.  However, when we are working with a lot of data, we might have a whole column of SMILES strings that we need to use to make molecule objects.  Further, we would like to save those molecule objects as a new column in our pandas dataframe.  This is generally true; you often want to calculate a new column of data using an existing column in your data frame.  The way to accomplish this is to use the `.apply` method.  You access any exisiting column of your python dataframe, put `.apply()` and then in the parenthesis, list a python function that calculates the thing you want to calculate.  In the code below, we will take the column of SMILES strings and apply the `Chem.MolFromSmiles` function and save the results as a new column of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO Insert Chem.MolFromSmiles in the brackets below\n",
    "# visualizing all the molecules in our data set\n",
    "df['mol'] = df['smiles'].apply(Chem.MolFromSmiles)\n",
    "df.head()\n",
    "\n",
    "# If you can not see a rendering of the molecule in the mol column, click on the three dots on left hand side and select \"change presentation\" and select \"text/html\" from top of the screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDKit molecule objects have a number of methods we can use to get more information about the molecule. In the next few cells, we'll look at some methods that can tell us some things about the molecules we've created.  \n",
    "\n",
    "We can use the `.apply` function that we just discussed to apply these methods to our molecule objects and save the results in a new column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Molecular Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mol_weight'] =df['mol'].apply(Descriptors.MolWt)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculating number of rotatable bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO Find a function which can calculate the number of rotable bonds after the Chem.rdMolDescriptors. below\n",
    "df['rot_bonds'] =df['mol'].apply(Chem.rdMolDescriptors)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating logP\n",
    "\n",
    "This uses the Wildman-Crippen LogP value calculation, an atom-based scheme based on the values in the paper Wildman and G. M. Crippen JCICS 39 868-873 (1999) (https://pubs.acs.org/doi/10.1021/ci990307l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clogP'] =df['mol'].apply(Chem.Crippen.MolLogP)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating aromatic ratio\n",
    "\n",
    "The aromatic proportion was calculated in the original paper by dividing the number of aromatic atoms by the number of total atoms. Although there is not a function in RDKit that calculates this directly, we can calculate it by creating our own function that uses two existing RDKit functions to perform the calculation. Then we can use our new function and ``.apply`` to make a new column in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:49:45.746932Z",
     "start_time": "2024-06-26T16:49:45.735372Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## defining the function that will calculate the aromatic proportion\n",
    "def aromatic_calc(mol):\n",
    "    prop_aromatic = len(mol.GetAromaticAtoms())/mol.GetNumAtoms()\n",
    "    return prop_aromatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO Insert aromatic_calc in the brackets below\n",
    "df['aromatic_ratio'] =df['mol'].apply(aromatic_calc)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other calculations RDKit can perform\n",
    "\n",
    "There are many other properties of molecules that RDKit can caculate.  In general, the methods in RDKit are organized into modules baesd on the type of property they calculate.  For instance, in some of the examples above, we used methods from the \n",
    "[`Descriptors` module](https://www.rdkit.org/docs/source/rdkit.Chem.Descriptors.html) and the [`rdMolDescriptors` module](https://www.rdkit.org/docs/source/rdkit.Chem.rdMolDescriptors.html).  You can click on either of those links to see the full list of the different properties you can access. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "    <b>Exercise</b>\n",
    "    Look through the documentation and the molecular properties you could\n",
    "    to add to your dataframe. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA) of the data set\n",
    "\n",
    "The next step after stating the problem and compiling the raw data is to perform exploratory data analysis (EDA) on raw data. The EDA is crucial for data preprocessing pipelines as it helps us understand the nature of our data, identify the key patterns and relationships, and detect anomalies. The EDA involves summarizing the main characteristics of the data, often using visual methods. \n",
    "\n",
    "Loading the data into a Pandas DataFrame provides a convenient way to perform EDA.\n",
    "\n",
    "In addition to looking for distribution and patterns in the data, look at what the columns actually contain. Some may include metadata about the source of the observation and its processing, which will not be relevant to the target variable, i.e. \"Compound ID\", \"smiles\" and \"mol\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO Type in df.info() below \n",
    "#Display informaion about the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1128 samples in the dataset with 6 columns and there appear to be no missing values.\n",
    "\n",
    "Each row corresponds to a compound. Two columns a contain identifier and molecular string representations.\n",
    "\n",
    "It is important to check any available documentation when you are working with a dataset that you have not compiled yourself. \n",
    "The AqSolDB paper/docs provide information about the dataset, such as which columns contain metadata regarding the source or processing of the data, the form of the quoted solubility values.\n",
    "\n",
    "The **target variable** is contained in the `logS` column. The paper tells us that this is a logS value, where S is the aqueous solubility in mol L<sup>-1</sup>.\n",
    "\n",
    "The majority of numerical columns are molecular descriptors.\n",
    "\n",
    "You should have some understanding of the nature of some features simply by looking at the names, e.g. (`mol_weight`, `rot_bonds`, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Missing Values\n",
    "\n",
    "Since the data is already preprocessed, the number of Missing values for all features should be zero. The experimentally measured solubility in the last column logS is the target variable and the remaining columns are the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "    <b>Note</b>\n",
    "    Always check for missing values in the data sets that you look at.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary of the Data\n",
    "We should also examine the statistical summary of each numerical features in our dataset. The  statistics include the value counts, mean, standard deviation, minimum, 25th percentile, median, average, 75th percentile, and maximum values for each feature.\n",
    "\n",
    "This information is extremely useful for understanding the data and the distribution of the features. It helps in identifying anomalies in the data or if our data requires any preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe dataset - we first create a new dataframe just with the numerical data dropping metadata\n",
    "df_data = df.drop(columns = ['Compound ID','smiles','mol'])\n",
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary statistics provide an overivew. There are a few points to note:\n",
    "\n",
    "1. Looking just at the mean (or median, the 50% percentile), the scale of the variables are quite different: e.g. clogP has a mean of 2.44 vs. mol_weight mean of 203.\n",
    "   \n",
    "2. The spread of values, indicated by the standard deviation and range, are also highly varied between variables.\n",
    "   \n",
    "3. The target variable (logS) has a negative mean and median, but the range extends into positive values.\n",
    "   \n",
    "4. Several variables have marked differences between the mean and median. From the summary statistics alone, you cannot definitively tell what is causing this. If you assume the distribution is unimodal, a difference between mean and median often points to asymmetry (skewness), and you could deduce the likely direction of the skew. However, the difference could also be caused by a multimodal distribution or the influence of outliers. Visualising the variable‚Äôs distribution would be a sensible step to explore this further.\n",
    "   \n",
    "5. The min and/or max values of several variables are far from the mean or median which may indicate that they are outliers. Again, visualisation of the distributions would provide more insight.\n",
    "   \n",
    "6. It is not the case here, but if a variable has a very small standard deviation, it suggests that it may be almost constant across the dataset. This might indicate low variability (which could be natural for some properties) or that a variable was not meaningfully measured. It is worth checking if any variables do have low variance, as they may contribute limited useful information to the analysis and could potentially be removed later. To explore this in more detail, you can scale the data and check for low variance/std deviation across the variables.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distributions of individual variables (univariate distributions)\n",
    "\n",
    "Visualising the distributions of variables is often the most straightforward way to get an initial assessment of the [shape](https://www.statsref.com/HTML/index.html?measures_of_distribution_shape.html) of an individual variable's distribution, but there are some numerical measures that can also provide an indication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness and Kurtosis\n",
    "The [`skewness`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.skew.html)  of the data, which is determined by comparing the mean and median values, should be looked at.\n",
    "\n",
    "The numerical values of skewness can be interpreted using the following rules:\n",
    "- The skewness value of zero indicates a perfect symmetrical distribution,\n",
    "- a skewness between -0.5 and 0.5 indicates an approximately symmetric distribution,\n",
    "- a skewness between -1 and -0.5 (or 0.5 and 1) indicates a moderately skewed distribution,\n",
    "- a skewness between -1.5 and -1 (or 1 and 1.5) indicates a highly skewed distribution, and\n",
    "- a skewness less than -1.5 (or greater than 1.5) indicates an extremely skewed distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "    <b>Note</b>\n",
    "    Which descriptors have high or extremely skewed distributions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.skew(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`kurt`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.kurt.html) method gives a measure of the \"tailedness\" - how heavy the tails of the distribution of a set of values (the opposite is \"peakedness\"). \n",
    "\n",
    "It uses the Fisher definition where a value of 0 indicates a normal distribution, while a positive value indicates a distribution with heavier tails (flatter peak) and a negative value indicates a distribution with lighter tails (sharper peak)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.kurt(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial visualisation of the distributions of features\n",
    "n_cols = 4  \n",
    "n_rows = (len(df_data.columns) + n_cols - 1) // n_cols  \n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3 * n_rows))\n",
    "\n",
    "# Flatten the axes array to iterate easily\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(df_data.columns):\n",
    "    sns.histplot(df_data[col], kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "\n",
    "# Hide any empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this scale, it is rather difficult to see the individual distributions clearly, but it is obvious that some of variables have very long \"tails\", pointing to extreme values - which could be potential outliers - in the data.\n",
    "\n",
    "For now, we can look more closely at the target variable. There are several possibilities, e.g. as a histogram, violinplot or boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(8, 4), sharey=True)\n",
    "\n",
    "sns.boxplot(y=\"logS\", data=df_data, ax=axes[0])\n",
    "sns.violinplot(y=\"logS\", data=df_data, ax=axes[1])\n",
    "\n",
    "axes[0].set_title('Box plot of Solubility values')\n",
    "axes[1].set_title('Violin plot of Solubility values')\n",
    "\n",
    "axes[0].set_ylabel('Solubility (log S)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These visualisations emphasise different aspects of the distribution. \n",
    "\n",
    "The [histogram](https://seaborn.pydata.org/generated/seaborn.histplot.html#seaborn.histplot) shows a detailed view of the frequency of subranges of the solubility values. It can be plotted with an overlaid line showing a [kernel density estimation](https://scikit-learn.org/stable/modules/density.html#kernel-density-estimation) (KDE) to provide a smoothed approximation of the underlying distribution (see also: [KDE pitfalls](https://seaborn.pydata.org/tutorial/distributions.html#kernel-density-estimation-pitfalls) for why combining the two can be valuable).\n",
    "\n",
    "The [boxplot](https://seaborn.pydata.org/generated/seaborn.boxplot.html) makes it straightforward to see the quartiles of the distribution and therefore highlights the presence of extreme values, but without much detail of the underlying values.\n",
    "\n",
    "The [violinplot](https://seaborn.pydata.org/generated/seaborn.violinplot.html) combines the information of the boxplot with a KDE to show more detail of the underlying distribution.\n",
    "\n",
    "The solubility distribution has a larger peak, centred around log‚ÄØS ‚âà ‚Äì2.5, with a shoulder at more negative values. This shows how visualisation can reveal important structure in the data that may not be apparent from simple summary statistics such as skewness. Recognising this complexity is important, as it may indicate the presence of subsets of compounds or differing underlying mechanisms that a single model might struggle to capture effectively.\n",
    "\n",
    "Various features of the box-, and violinplots can be adjusted - see the documentation. They convey the information about a single variable's distribution in a compact form, so can be particularly useful to quickly do this for several variables at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bivariate analysis - pairwise relationships\n",
    "\n",
    "A correlation value close to 1 between a pair of descriptors indicates a strong positive relationship, while a correlation value close to -1 indicates a strong negative relationship. A correlation value close to 0 indicates no relationship between the features.\n",
    "\n",
    "The coloring scheme makes it easy to uncover the relationships between the features. The darker the color, the stronger the correlation. The diagonal line represents the correlation of each feature with itself, which is always 1. The blue color indicates a positive correlation, while the red color signifies a negative correlation (based on the provided key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO Type in df_corr.corr() in line 3 below\n",
    "# Caculate the correlation matrix - by default, this will use Pearson's correlation coefficient, r\n",
    "corr = \n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial visualisation of the correlations between variables\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Create a mask to hide the upper triangle - makes the heatmap easier to read\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "sns.heatmap(corr, \n",
    "            annot=True, fmt=\".2f\", \n",
    "            cmap='coolwarm', \n",
    "            square=True,\n",
    "            mask=mask, \n",
    "            cbar_kws={'label': 'Pearson correlation coefficient, r'})\n",
    "\n",
    "plt.title('Correlation matrix of features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix clearly shows strong bivariate relationships between several features. For example, the magnitude of the Pearson correlation coefficient is above 0.8 clogP and logS.\n",
    "\n",
    " Strongly correlated features are potential candidates for removal if you are considering reducing the number of features in your model (feature selection). This is because strongly correlated features contribute overlapping information, so including both can make one redundant. Using fewer features can improve the interpretability of the model and may also improve its performance by reducing overfitting.\n",
    "\n",
    "Detecting strongly correlated features is particularly important in regression models, where high [multicollinearity](https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/) (when two or more features are highly correlated) can cause problems. Multicollinearity makes it difficult to estimate the effect of each feature accurately and can lead to unstable or misleading model coefficients.\n",
    "\n",
    "If you are using exploratory data analysis to guide feature selection, it is important to remember that this should be based only on the training data, not the full dataset. Using information from the test data at this stage would lead to data leakage, giving an unrealistically optimistic estimate of model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of Features with Target Variable\n",
    "The correlation matrix between the features and the target variable provides insights into the relationships between the features and the target variable(s). \n",
    "\n",
    "A correlation value close to 1 indicates a strong positive relationship, while a correlation value close to -1 indicates a strong negative relationship. A correlation value close to 0 indicates no relationship between the features.\n",
    "\n",
    "The strongest correlation with the solubility values is with cLogP, indicated by a moderately negative Pearson correlation coefficient of -0.83. It makes sense that aqueous solubility and cLogP would be related and in the inverse direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise relationships between features and target\n",
    "\n",
    "n_cols = 4  \n",
    "n_rows = (len(df_data.drop(columns=[\"logS\"]).columns) + n_cols - 1) // n_cols  \n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3 * n_rows), sharey=True)\n",
    "\n",
    "\n",
    "# Flatten the axes array to iterate easily\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(df_data.drop(columns=[\"logS\"]).columns):\n",
    "\n",
    "    # Use regplot to add a regression line to the scatter plot of target vs. feature\n",
    "    sns.regplot(data=df_data, \n",
    "                x=col, \n",
    "                y=\"logS\", \n",
    "                marker=\".\",\n",
    "                ci=None,\n",
    "                line_kws=dict(color=\"r\", linewidth=1),\n",
    "                scatter_kws=dict(alpha=0.5, linewidths=0.5),\n",
    "                ax=axes[i])\n",
    "\n",
    "    # You can also try to colour the points by another feature to see if there is a > bivariate relationship, e.g.:\n",
    "    # sns.scatterplot(data=num_sol_df, x=col, y=\"Solubility\", hue=\"HeavyAtomCount\", ax=axes[i], palette=\"viridis\")\n",
    "    axes[i].set_title(f'Log S vs. {col}')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Solubility (log S)')\n",
    "    axes[i].set_ylim(df_data[\"logS\"].min() - 1, df_data[\"logS\"].max() + 1)\n",
    "\n",
    "\n",
    "# Hide any empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots also show that for some of the variables there is not a strong linear relationship between the descriptor and the compound solubility. This suggests that any dependence may be more complex: nonlinear, involving combinations of features, or conditional on particular regions of the data. For example, solubility might increase with molecular weight up to a point, but then decrease for very large molecules - a relationship that would not be captured well by a simple straight line. \n",
    "\n",
    "When linear relationships are weak, this can be one reason to explore nonlinear models or feature engineering to better capture patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points to Remember:\n",
    "  EDA can help to uncover initial information on:\n",
    "\n",
    "   - Feature distributions\n",
    "     - Check descriptive statistics  \n",
    "     - mean, median, standard deviation, range, quartiles (e.g., using `df.describe()`)\n",
    "   - Visualise using plots like\n",
    "     - Histograms (shape, skewness, modality)\n",
    "     - Box plots (spread, outliers)\n",
    "     - KDE plots (smooth estimate of distribution shape)\n",
    "    \n",
    "   - Relationships between variables\n",
    "     - Numerical summaries for multiple pairwise relationships\n",
    "     - Correlation matrix (e.g., Pearson correlation coefficients)\n",
    "   - Visualise using plots like\n",
    "     - Scatter plots (direct relationships between two variables)\n",
    "\t  - Correlation heatmaps (summarising pairwise correlations visually)\n",
    "  \n",
    "   - Detecting outliers or unusual patterns\n",
    "     - Look for\n",
    "\t   - Large differences between mean and median, extreme minimum or maximum values, unusually wide ranges.\n",
    "   - Visualise using plots\n",
    "     - Box plots (to highlight points outside the whiskers)\n",
    "\t   - Scatter plots (isolated points far from main clusters)\n",
    "\t   - Histograms/KDE plots (unexpected pronounced tails)\n",
    "\n",
    "   - Checking for Redundancy and Multicollinearity\n",
    "\t   - Look for\n",
    "\t   - High correlation coefficients between features (e.g., > 0.9 or < -0.9).\n",
    "\t - Visualise using plots\n",
    "\t   - Correlation heatmaps (visually detect highly correlated pairs).\n",
    "\t\n",
    "   - Some modelling choices informed by EDA:\n",
    "\t    - Remove irrelevant metadata features\n",
    "\t    - Consider dropping or combining highly correlated features\n",
    "\t    - Highlight features with very low variance for possible removal\n",
    "\n",
    "  Overall, EDA can provide information on the following questions:\n",
    "   - Are there obvious problems with the data (outliers, skewed distributions) that might require addressing before modelling?\n",
    "   - Which features might be the most informative or redundant?\n",
    "   - What kind of models might work best (e.g., linear models vs. nonlinear models)?\n",
    "   - How should the data be pre-processed before training models?\n",
    "\n",
    "    EDA does not provide comprehensive answers from the data, but by exploring and gaining better understanding your data, it allows you to ask much better questions about your data, the features and about your modelling approach.\n",
    "\n",
    "  \n",
    "  - Some feature-to-feature linear correlations are present, indicated by the Pearson correlation coefficients and the bivariate scatterplots.\n",
    "\n",
    "\t- No single feature shows very strong correlation with the target; the strongest linear relationship between solubility and CLogP. Given that the solubility here is aqueous solubility (log S), the observed moderate negative correlation with MolLogP (the n-octane-water partition coefficient) would be chemically reasonable.\n",
    "\t\n",
    "\t- It is important to remember that a lack of strong linear correlation does not necessarily indicate there is no relationship: complex, nonlinear or interaction effects may exist.\n",
    "\n",
    "\t- Metadata features not chemically relevant to solubility (e.g., Occurrences, Group) should be removed.\n",
    "\t- Strongly correlated features might be candidates for removal. Strong correlations can mean that some variables do not contribute additional data, i.e. they are redundant. Removing redundant features can improve model interpretability and reduce risk of overfitting. \n",
    "\t- Features with little variation (very low variance/standard deviation) would be considered for removal although none were found here. Again, this can enhance model interpretability and reduce model complexity.\n",
    "\t- Care must be taken to select features based only on training data to avoid data leakage.\n",
    "\n",
    "- Note the distributions of values of features (e.g. the measures of centre, the magnitude and shape of the distribution and the range of the values).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data Into Training and Testing Sets\n",
    "\n",
    "Once we have a good understanding of the data, we can move on to the next step, which is splitting the data into a training set and a testing set. The training set is used to train the model, and the testing set is used to evaluate the model's performance. This process allows you to test the model's accuracy on unseen data and ensures that the model can generalize well to new data.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note</b>\n",
    "It is extremely important to split the data first and then perform subsequent feature engineering steps. Feature engineering prior to splitting the data can cause a <b>data leakage</b> problem, allowing the model to \"see\" the testing data in the training phase. This violates our intention to treat the test data as a good representative sample of the real-world data. Data leakage leads to a model that performs well in training and testing but that performs poorly when given novel data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our training and testing data sets, we will use the ``train_test_split`` function from the ``sklearn.model_selection`` module to split the data. The training set will be used to train the model, while the testing set will be used to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful information about the train test split process:\n",
    "- ``X`` generally denotes the input variables (the data the model will use to make predictions)\n",
    "- ``y`` is often used for target variable (what we are trying to predict)\n",
    "- ``test_size`` is used to assign the percentage of the data set aside for the testing set\n",
    "- ``random_state`` controls the random number generator used to shuffle the data before splitting it. In other words, it ensures that the same randomization is used each time you run the code, resulting in the same splits of the data. This is especially useful if you want to compare the performance of multiple models\n",
    "- ``shuffle = True`` ensures that the data is split randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO Add clogP in between the \"\" in line 4 and logS between '' in line 5 \n",
    "# Create the feature matrix (X), feature vector (x), and the target vector (y)\n",
    "X = df.drop(columns=['logS'])\n",
    "x = X[\"\"]\n",
    "y = df['']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123, shuffle=True)\n",
    "\n",
    "# Reshape the data into 2D arrays of shape (n_samples, 1)\n",
    "# (if working with only one input feature)\n",
    "x_train = x_train.values.reshape(-1,1)\n",
    "x_test = x_test.values.reshape(-1,1)\n",
    "y_train = y_train.values.reshape(-1,1)\n",
    "y_test = y_test.values.reshape(-1,1)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note</b>\n",
    "    Note the use of the <b>x</b> vector in the <b>train_test_split</b> function as initially we will train a model using only one input feature <b>CLogP</b>.<br> If we instead want to use all the available features we would need to use the <b>X</b> matrix that we have defined.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Reminder:</b>\n",
    "    It is extremely important to split the data first and then fit the scaler on the training data, only. Fitting the scaler on the entire data and then splitting it causes the <b>data leakage</b> problem which violates our intention to treat the test data as a good representative sample of the real-world data.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Once we have a good understanding of the data, we can move on to the next step, which is feature engineering. Feature engineering is the process of transforming the raw data into a format that is suitable for machine learning models. Feature engineering often involves creating new features, selecting the most important features, and transforming the existing features in order to improve the model's performance.\n",
    "\n",
    "After splitting our data, we need to scale our train and test features. Scaling is a crucial step in the data preprocessing pipeline as it ensures that all features have the same scale, since many machine learning models are sensitive to the scale of the input features. We will use the ``StandardScaler`` from the ``sklearn.preprocessing`` module to scale our features. ``StandardScaler`` transforms the data in such a manner that it has mean value of 0 and a standard deviation value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the standard scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training feature vector x_train\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "\n",
    "# Transform the test feature vector x_test\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Make sure the training data is scaled correctly\n",
    "print(f\" Training feature mean: {x_train_scaled.mean():.5f}\")\n",
    "print(f\" Training feature standard deviation: {x_train_scaled.std():.5f}\\n\")\n",
    "\n",
    "# Print the scaler statistics on the test data\n",
    "print(f\" Testing feature mean: {x_test_scaled.mean():.5f}\")\n",
    "print(f\" Testing feature standard deviation: {x_test_scaled.std():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training a Linear Regression Model\n",
    "\n",
    "The next step after the data preparation is to build and train our model. We will build a simple linear regression model which focuses on the relationship between a single feature (``CLogP``) and the target variable (``logS``). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "    <b>Consider</b>\n",
    "    What is the reason behind choosing <b>CLogP</b> as our main feature in the linear regression model?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the performance of our model, we can first create a dummy \"model\" using the ``DummyRegressor`` class from the ``sklearn.dummy`` module. This class provides a simple way to create a model that calculates the mean value of the target feature and predicts this mean value for each observation. The ``fit`` method is used to train the model on the training data. Once the model is trained, we can use the ``predict`` method to make predictions on the test data. Note that the ``DummyRegressor`` is not for solving real problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy model using the mean value of the target property\n",
    "dummy_model = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "# TO DO Add in y_train in the brack below after the comma\n",
    "# Fit the model to the training data\n",
    "dummy_model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_dummy = dummy_model.predict(x_test_scaled)\n",
    "\n",
    "# Calculate the performance metrics and store them in a DataFrame\n",
    "dummy_results = pd.DataFrame({\n",
    "    \"Coefficients\": [np.array(dummy_model.constant_)],   # the regression coefficient\n",
    "    \"MSE\": mean_squared_error(y_test, y_pred_dummy),     # the mean squared error\n",
    "    \"R2\": r2_score(y_test, y_pred_dummy)                 # the coefficient of determination\n",
    "}, index=[\"Dummy\"])\n",
    "dummy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "    <b>Consider</b>\n",
    "    Do we want to maximize or minimize the MSE value? What about R2?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model\n",
    "\n",
    "Let's next build and train a single-feature input linear regression model. We will use the ``LinearRegression`` class from the ``sklearn.linear_model`` module to create the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple linear regression model\n",
    "simple_reg_model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "simple_reg_model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_simple = simple_reg_model.predict(x_test_scaled)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "simple_model_results = pd.DataFrame({\n",
    "    \"Coefficients\": [np.array(simple_reg_model.coef_)],   # the regression coefficient\n",
    "    \"MSE\": mean_squared_error(y_test, y_pred_simple),     # the mean squared error\n",
    "    \"R2\": r2_score(y_test, y_pred_simple)                 # the coefficient of determination\n",
    "}, index=[\"Simple-Linear-Regression\"])\n",
    "\n",
    "# Store the results into results DataFrame\n",
    "results = pd.concat([dummy_results, simple_model_results])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot object \n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Plot the test data\n",
    "ax.scatter(x_test_scaled, y_test, color='blue', label='Test Data')\n",
    "\n",
    "# Plot the simple linear regression model\n",
    "ax.plot(x_test_scaled, y_pred_simple, color='red', label='Simple Linear Regression')\n",
    "\n",
    "# Plot the baseline model\n",
    "ax.plot(x_test_scaled, y_pred_dummy, \"g--\", label=\"Dummy\")\n",
    "\n",
    "# Create the legends\n",
    "fig.legend(facecolor='white')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a multifeature linear regression\n",
    "\n",
    "We can improve our model by including other features that show some correlation with the target variable. This is where multiple linear regression comes in.\n",
    "\n",
    "Let's build a multiple linear regression model using all the features in our dataset. The process is very similar to building a single-feature linear regression model: Once again, we need to scale the data, train the model on the scaled training data using the ``fit`` method, and make predictions on the test data using the ``predict`` function.\n",
    "\n",
    "Linear regression models the relationship between input features and a continuous target variable using a linear function (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "The function looks like:  \n",
    "\n",
    "$$\n",
    "y = w_0 + w_1x_1 + w_2x_2 + \\dots + w_nx_n + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $ y $ = **Predicted output** (target variable)  \n",
    "- $ x_1, x_2, \\dots, x_n $ = **Input features** (independent variables)  \n",
    "- $ w_0 $ = **Intercept** (bias term)  \n",
    "- $ w_1, w_2, \\dots, w_n $ = **Coefficients** (weights)  \n",
    "- $ \\epsilon $ = **Error term** (accounts for noise in data)  \n",
    "\n",
    "The goal is to find weights $w_{i}$ that minimize the error, typically using Ordinary Least Squares (OLS).\n",
    "\n",
    "It finds a best-fit line by minimising the difference between predictions and actual values, typically using least squares. \n",
    "\n",
    "It is widely used for trend analysis, forecasting, and understanding feature impact on outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "\n",
    "We will again split the data into train and test prior to doing any other data cleaning or engineering, to prevent data leakage between the training and testing data. We will use the ``train_test_split`` function from the ``sklearn.model_selection`` module to split the data into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate the model's performance. We will also use a ``random_state `` again, so the data is the same when comparing different models. \n",
    "\n",
    "We will drop the target vector 'logS', and also the 'smiles', 'mol'  and 'Compounds ID' columns as we do not want the model to use those to predict the solubility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature matrix (X) and target vector (y)\n",
    "X = df.drop(columns=['logS', 'smiles', 'mol', 'Compound ID'])\n",
    "y = df['logS']\n",
    "\n",
    "#TO DO Add in 0.2 as the test_size\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=, random_state=123, shuffle=True)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note</b>\n",
    "    Note the usage of <b>X</b> matrix instead of <b>x</b> vector in the <b>train_test_split</b> function, since now we want to use multiple features to predict the target variable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting our data, we need to scale our training (and test) features. Scaling is a crucial step in the data preprocessing pipeline as it ensures that all features have the same scale as many machine learning models are sensitive to the scale of the input features. We will use the ``StandardScaler`` from the ``sklearn.preprocessing`` module to scale our features. Note that a Random Forest model does not require data scaling, as it is a tree-based model and so different scales will not affect model performance. We will still scale the data, however, so we can also build a linear regression model and compare the performance of two models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the standard scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training feature vector x_train\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "\n",
    "# Transform the test feature vector x_test\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# TO DO Copy lines of code from above where we used feature engineering to examine mean and SD of training and test sets \n",
    "# Make sure the training data is scaled correctly\n",
    "\n",
    "\n",
    "# Print the scaler statistics on the test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Reminder:</b>\n",
    "    It is extremely important to split the data first and then fit the scaler on the training data only. Fitting the scaler on the entire data and then splitting it can cause a <b>data leakage</b> problem which violates our intention to treat the test data as a good representative sample of the real-world data.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "We create a new LinearRegression model and train it using its `fit` method on the training data's features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regression model\n",
    "multi_feature_model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "multi_feature_model.fit(x_train_scaled, y_train)\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model's performance on unseen data\n",
    "\n",
    "You can now get the model to predict the solubilities for the subset of data you withheld for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO Add x_test_scaled in the brackets below\n",
    "# Make predictions on the test data\n",
    "y_pred_linear_multi = multi_feature_model.predict()\n",
    "\n",
    "# Calculate the performance metrics and store them in a DataFrame\n",
    "multi_results = pd.DataFrame({\n",
    "    \"MSE\": mean_squared_error(y_test, y_pred_linear_multi),     # the mean squared error\n",
    "    \"R2\": r2_score(y_test, y_pred_linear_multi)                 # the coefficient of determination\n",
    "}, index=[\"Multi-Linear-Regression\"])\n",
    "\n",
    "# Store the results into results DataFrame\n",
    "results = pd.concat([results, multi_results])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "    <b>Consider</b>\n",
    "    How much did model performance improve by using a multi-linear model over the linear model? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points to remember\n",
    "\n",
    "- **Metrics that can be useful for evaluating the model's performance**\n",
    "\n",
    "  A quick recap on $R^2$  (coefficient of determination):\n",
    "\n",
    "  $R^2$ measures the proportion of the total variance in the target variable that is captured by the model‚Äôs predictions. It compares the variance of the model‚Äôs residuals (errors) to the variance in the actual data.\n",
    "\n",
    "\t$R^2$ = 1: All the variance in the target is explained ‚Äî predictions fall exactly on the true values.\n",
    "\n",
    "\t$R^2$ = 0: None of the variance is explained ‚Äî predictions are no better than always predicting the mean.\n",
    "\n",
    "\t$R^2$ < 0: The model explains less variance than the mean prediction ‚Äî worse than guessing the average.\n",
    "\n",
    "  $R^2$ is unitless and focuses on how well the model explains the spread (variance) of the target values, but it does not directly indicate how close the predictions are to the true values. Metrics like MAE and RMSE, which measure average error size, express that aspect of the performance.  \n",
    "\n",
    "\n",
    "  Other metrics for regression:\n",
    "\n",
    "  -\t**Mean absolute error** (MAE)\n",
    "\n",
    "    The average absolute difference between predicted and actual values.\n",
    "\n",
    "    $$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "    Where\n",
    "\t    $y_i$ = true (actual) value,\n",
    "\t    $\\hat{y}_i$ = predicted value,\n",
    "\t    $n$ = number of data points\n",
    "\n",
    "    A smaller MAE indicates better performance: there is less difference between predictions and true values on average.\n",
    "\n",
    "    It gives the average magnitude of the errors, without considering their direction (i.e. positive or negative). It is expressed in the same units as the target variable, so it is useful for interpreting typical prediction error in real terms, unlike MSE.\n",
    "\n",
    "    <br/>\n",
    "  - **Root mean square error** (RMSE)\n",
    "\n",
    "    Square root of the average squared differences between predicted and actual values.\n",
    "\n",
    "    $$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "    A smaller RMSE indicates better performance.\n",
    "    \n",
    "    RMSE has the same units as the target variable (e.g. log‚ÄØS), making it easy to interpret the typical size of errors. It is more sensitive to large errors than MAE (because it squares them).\n",
    "\tUnlike $R^2$, RMSE gives a direct sense of how far off the predictions are on average.\n",
    "\n",
    "    <br/>\n",
    "  - **Mean Squared Error** (MSE) \n",
    " \n",
    "    Average of the squared errors ‚Äî less interpretable than RMSE but useful analytically.\n",
    "\n",
    "    $$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "    Squaring errors emphasises larger errors, making MSE sensitive to outliers. The units of MSE are the square of the target variable‚Äôs units, so it is less interpretable than MAE or RMSE. However, it is analytically convenient, particularly for optimisation and model comparison.\n",
    "\n",
    "    <br/>\n",
    "\n",
    "  $R^2$ is useful for understanding relative performance and model fit.\n",
    "\n",
    "  RMSE and MAE are better for interpreting absolute error sizes.\n",
    "\t\n",
    "  Reporting both $R^2$ and a direct error metric, like RMSE or MAE, gives a more complete picture of model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported from sklearn.metrics\n",
    "from sklearn.metrics import (root_mean_squared_error,\n",
    "                            mean_absolute_error,\n",
    "                            mean_squared_error,\n",
    "                            accuracy_score, \n",
    "                            classification_report, \n",
    "                            confusion_matrix,\n",
    "                            ConfusionMatrixDisplay,\n",
    "                            roc_auc_score)\n",
    "\n",
    "metrics = [mean_absolute_error, \n",
    "          root_mean_squared_error, \n",
    "          mean_squared_error]\n",
    "\n",
    "metric_names = [\"MAE\", \"RMSE\", \"MSE\"]\n",
    "\n",
    "y_train_pred = multi_feature_model.predict(x_train_scaled)\n",
    "\n",
    "print(\"Metrics for multi-linear regression\")\n",
    "\n",
    "for metric, name in zip(metrics, metric_names):\n",
    "    train_score = metric(y_train, y_train_pred)\n",
    "    test_score = metric(y_test, y_pred_linear_multi)\n",
    "    print(f\"{name} train: {train_score:.3f} \\t{name} test: {test_score:.3f}\")\n",
    "\n",
    "\n",
    "print(f\"R^2 train: {r2_score(y_train, y_train_pred) :.3f} \\tR^2 test: {r2_score(y_test, y_pred_linear_multi) :.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error metrics for multi-linear regression prediction of solubility (log S):\n",
    "\n",
    "\tMAE (~0.8) indicates the model‚Äôs average absolute prediction error in logS units, which gives an intuitive sense of typical error size.\n",
    "\n",
    "\tRMSE (~1.0) is slightly higher than the MAE, as expected, due to squaring larger errors, but the small difference suggests that no extreme outliers dominate the error distribution.\n",
    "  \n",
    "\tThe similarity of RMSE and MAE also implies that errors are fairly evenly spread and not skewed by a few very large deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model performance for multi-linear regression**\n",
    "\n",
    "  $R^2$ values: ~0.77 (train), ~0.77 (test) suggest moderate predictive power. MAE, RMSE, MSE are all very similar.\n",
    "  $R^2$ indicates that the simple linear regression model captures some but not all of the variance in solubility.\n",
    "  The small gap between train and test metrics suggests the model generalises reasonably, with low risk of overfitting.\n",
    "\n",
    "  Linear regression assumes a linear relationship between the input features and the target (logS). However, molecular solubility is likely influenced by complex, non-linear combinations of structural and physicochemical descriptors. The moderate R^2 values suggest that the model captures some trends but fails to account for the full variability in the data. This supports the idea that linear models may be too simple to represent the underlying structure‚Äìproperty relationships. Given the complexity of the problem and the simplicity of the model, underfitting is more likely than overfitting.\n",
    "\n",
    "<br/>\n",
    "\n",
    "- **Examine the model coefficients for multi-linear regression**\n",
    "\n",
    "  The size of each model coefficient depends on how important the feature is, but also on its units and scale, so if the features have very different ranges, this will make it more difficult to interpret the relative importance of the features based on the coefficients. A scaler (here [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)) can be used to tranform the features so they have the same scale (usually mean = 0 and variance = 1): this process is called **standardisation**. After scaling, the values of the features are normalised to the same scale, so feature importance can be compared independently of their original ranges.\n",
    "\n",
    "  After feature scaling, the size and sign of each coefficient indicates the relative influence of each feature on the prediction. The sign indicates the direction of the relationship, similar to correlation coefficients like r. The magnitude of the coefficient tells you about the relative importance of the features to the overall model. \n",
    "\n",
    "  Model coefficients can guide decisions such as feature selection; e.g. very small values may indicate features that contribute little and could be removed. However, interpretation must be considered in the context of the chemistry: do the signs and magnitudes make sense? If not, this may suggest issues such as multicollinearity or model limitations, rather than genuine relationships. Coefficients should be interpreted critically and in context, supported by performance metrics and other diagnostics, to understand both the model and the system it aims to describe.\n",
    "\n",
    "  The linear regression coefficients for the aqueous solubility prediction task provide a useful example of how model outputs must be interpreted critically. The EDA indicated that several of the input features are strongly correlated, and the coefficients likely reflect these interdependencies rather than the true underlying importance of each feature.\n",
    "\n",
    "  From the EDA, we saw that several features are correlated. As discussed earlier, this can lead to multicollinearity, which can result in coefficients become unstable or misleading. This can make it difficult to disentangle the individual effects of correlated variables: the model's interpretability is impaired. It may also increase the risk of model overfitting.\n",
    "\n",
    "  The linear regression model coefficients may not give a true impression of the importance of the correlated features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept:\",multi_feature_model.intercept_)\n",
    "coeff_df = pd.DataFrame({\"Feature\": x.columns, \"Coefficient\": multi_feature_model.coef_})\n",
    "print(\"\\nFeature Coefficients:\\n\", coeff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training a Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a random forest model?\n",
    "The next step after the data preparation is to build and train our random forest regression model (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). A random forest is a decision tree model that uses a \"forest\" of multiple decision trees and randomly chooses which variables to use in each tree. Generally, each individual tree is not that good at making a prediction, but collectively the trees are quite good at making predictions. Note that each tree predicts an individual value and then a vote is taken (in the case of regression, an average of the predicted values) to determine the final predicted value.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Random_forest_explain.png\" style=\"display: block; margin: 0 auto; max-height:400px;\">\n",
    "</div>\n",
    "\n",
    "### Hyperparameters\n",
    "Random forest models (and many other models) have hyperparameters that can be 'tuned' to opmitize model performance.The number of trees (``n_iterators``) can be specified, with a default value of 100 trees. Each tree also has a depth(``max_depth``) which specifies the maximum number of splits a tree can have, with the default value being no limit on the depth ('None'). Too few trees or trees that are too shallow results in a model that predicts poorly due to underfitting the data--the model is too simple to predict using train or test data. Too many trees or trees that are too deep results in a model that predicts poorly due to overfitting the data--the model is so complex it can predict using the original data extremely well but cannot predict using new data.\n",
    "\n",
    "### Building and training a default random forest regression model\n",
    "We will first build and train a random forest model that uses the default parameters. We will use the ``RandomForestRegressor`` class from the ``sklearn.ensemble`` module to create the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO Add in RandomForestRegressor() to the right of the equals sign below\n",
    "# Create a random forest regression model\n",
    "default_rf_model = \n",
    "\n",
    "# Fit the model to the training data\n",
    "default_rf_model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_default_rf = default_rf_model.predict(x_test_scaled)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "default_rf_model_results = pd.DataFrame({\n",
    "    \"MSE\": mean_squared_error(y_test, y_pred_default_rf),     # the mean squared error\n",
    "    \"R2\": r2_score(y_test, y_pred_default_rf)                 # the coefficient of determination\n",
    "}, index=[\"Default_RF_Regression\"])\n",
    "\n",
    "# Store the results into results DataFrame\n",
    "results = pd.concat([results, default_rf_model_results])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "    <b>Consider</b>\n",
    "    Which model is best at predicting the value of logS?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:52:00.069272Z",
     "start_time": "2024-06-26T16:51:59.817331Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a plot object\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Plot the test data\n",
    "ax.scatter(y_test, y_pred_simple, color='blue', label='Linear Regression Test')\n",
    "\n",
    "# Plot the multi-variable linear regression model\n",
    "ax.scatter(y_test, y_pred_linear_multi, color='green', label=\"Multifeature Linear Regression Test\")\n",
    "\n",
    "# TO DO Add in y_pred_default_rf in the expression below\n",
    "# Plot the simple linear regression model\n",
    "ax.scatter(y_test, y_pred_default_rf, color='red', label='Random Forest Regression Test')\n",
    "\n",
    "# Create the legends\n",
    "fig.legend(facecolor='white')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance in Random Forests**\n",
    "\n",
    "There are several ways of exmaining feature importance. We will look at two: Gini importance and SHAP\n",
    "\n",
    "*Gini Importance*\n",
    "\n",
    "The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. The criterion is the Gini impurity, which measures the impurity of a node in a decision tree, with more substantial weight to the most important features. Therefore, Gini importance is also known as the total decrease in node impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in feature importance (Gini Importance)\n",
    "importances = default_rf_model.feature_importances_\n",
    "feature_imp_df = pd.DataFrame({'Feature': x_train.columns, 'Gini Importance': importances}).sort_values('Gini Importance', ascending=False) \n",
    "print(feature_imp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot for feature importance\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.barh(x_train.columns, importances, color='skyblue')\n",
    "plt.xlabel('Gini Importance')\n",
    "plt.title('Feature Importance - Gini Importance')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHAP**\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) (https://www.geeksforgeeks.org/machine-learning/shap-a-comprehensive-guide-to-shapley-additive-explanations/) value is a measure of the contribution of a feature towards the prediction for each instance.\n",
    "\n",
    "For a given prediction made by a Random Forest model, SHAP values can be calculated for each feature for that specific instance.\n",
    "It involves considering all possible subsets of features and their interactions, measuring the impact of including or excluding each feature on the prediction.\n",
    "Positive SHAP values indicate a positive contribution to the prediction, while negative values suggest a negative contribution. The magnitude of the SHAP value represents the strength of the contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# TO DO Add in default_rf_model below\n",
    "explainer = shap.Explainer()\n",
    "shap_values = explainer.shap_values(x_test_scaled)\n",
    "shap_summary = np.abs(shap_values).mean(axis=0) \n",
    "shap_summary_df = pd.DataFrame({'Feature': X.columns, 'SHAP values': shap_summary})\n",
    "shap_summary_df = shap_summary_df.sort_values('SHAP values', ascending=False)\n",
    "print(shap_summary_df)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(shap_summary_df['Feature'], shap_summary_df['SHAP values'], color='skyblue')\n",
    "plt.xlabel('Mean Absolute SHAP Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance based on SHAP Values')\n",
    "plt.gca().invert_yaxis() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning a Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first Random Forest model used the default hyperparameter settings. Tuning (optimizing) hyperparameter settings can improve model performance. We will use a loop to try a range of hyperparameter settings with our model. Although we don't have time to use them today, tools such as ``GridSearchCV`` and ``Optuna`` can be used help identify the best set of hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the values to test for the n_iterators and max_depth hyperparameters\n",
    "trees = [50, 100, 300, 500, 1000]\n",
    "depths = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "## Defining lists to store results for each set of \n",
    "tree_count = []\n",
    "tree_depth = []\n",
    "tuned_rf_MSE = []\n",
    "tuned_rf_R2 = []\n",
    "\n",
    "for tree in trees:\n",
    "    \n",
    "    for depth in depths:\n",
    "        \n",
    "        # Create a random forest regression model\n",
    "        tuned_rf_model = RandomForestRegressor(n_estimators = tree, max_depth = depth)\n",
    "\n",
    "        # Fit the model to the training data\n",
    "        tuned_rf_model.fit(x_train_scaled, y_train)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        y_pred_tuned_rf = tuned_rf_model.predict(x_test_scaled)\n",
    "\n",
    "        # Storing the results in lists\n",
    "        tree_count.append(tree)\n",
    "        tree_depth.append(depth)\n",
    "        tuned_rf_MSE.append(mean_squared_error(y_test, y_pred_tuned_rf))\n",
    "        tuned_rf_R2.append(r2_score(y_test, y_pred_tuned_rf))\n",
    "        \n",
    "        \n",
    "# Create a DataFrame from the lists of results\n",
    "results_df = pd.DataFrame(\n",
    "    {'Number of trees': tree_count,\n",
    "     'Max depth': tree_depth,\n",
    "     'MSE': tuned_rf_MSE,\n",
    "     'R2': tuned_rf_R2\n",
    "    })\n",
    "\n",
    "# display the results DataFrame\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "    <b>Note</b>\n",
    "    Which set of hyperparameters gave the best model performance?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus material for more learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving model performance using cross-validation\n",
    "\n",
    "You evaluated the performance of our trained model on the test dataset. However, the measured performance depends on the quality of the data in the splits (train/validation/test).\n",
    "In order to ameliorate this issue, you can use a technique called $k$-fold cross-validation. The $k$-fold cross-validation method splits the data into $k$ subsets,\n",
    "trains the data on the union of $k-1$ sets and measures the performance of the trained model on the $k$-th set, and repeats the process $k$ times to cover all subsets.\n",
    "The performance score is reported as the average score of $k$ experiments.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"K-fold_cross_validation_EN.png\" style=\"display: block; margin: 0 auto; max-height:400px;\">\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "Here, you can use the ``cross_val_score`` function from the ``sklearn.model_selection`` module to perform a 5-fold cross validation experiment.\n",
    "\n",
    "You can use cross-validation to train both the multi-feature linear regression model and the random forest regression model. We will again use the ``cross_val_score`` function from the ``sklearn.model_selection`` module to perform a 5-fold cross validation experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "# Create a pipeline object\n",
    "pipeline_multi_feature_linear = make_pipeline(scaler, multi_feature_model)\n",
    "pipeline_default_rf = make_pipeline(scaler, default_rf_model)\n",
    "\n",
    "# Create a KFold object\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results_multi_feature_linear = cross_val_score(pipeline_multi_feature_linear, x, y, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "cv_results_default_rf = cross_val_score(pipeline_default_rf, x, y, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# Calculate the mean and standard deviation of the cross-validation results\n",
    "print(f\"CV Results Multifeature Linear Regression Mean MSE: {-cv_results_multi_feature_linear.mean():.5f} +/- {cv_results_multi_feature_linear.std():.5f} \")\n",
    "print(f\"CV Results Default Random Forest Mean MSE: {-cv_results_default_rf.mean():.5f} +/- {cv_results_default_rf.std():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "    <b>Consider</b>\n",
    "    Did using cross-validation improve the performance of the models?\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem521",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
